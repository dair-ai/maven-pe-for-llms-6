{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Paper Tagger\n",
    "\n",
    "Here what's provided in the notebook:\n",
    "- Compares zero-shot, few-shot, CoT, and self-consistency\n",
    "- Evaluates using an LLM evaluator using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# create a new LLM\n",
    "from langchain.llms import OpenAI\n",
    "llm  = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "def print_markdown(text):\n",
    "    \"\"\"Prints text as markdown\"\"\"\n",
    "    IPython.display.display(IPython.display.Markdown(text))\n",
    "\n",
    "# load json data at path: data/article-tags.json into a dataframe\n",
    "with open('../data/article-tags.json') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open('../data/few_shot.json') as f:\n",
    "    few_shot_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer example selector\n",
    "\n",
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples: List[Dict[str, str]]):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example: Dict[str, str]) -> None:\n",
    "        \"\"\"Add new example to store for a key.\"\"\"\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, size) -> List[dict]:\n",
    "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "        return np.random.choice(self.examples, size=size, replace=False)\n",
    "    \n",
    "\n",
    "example_selector = CustomExampleSelector(few_shot_data)\n",
    "\n",
    "template = \"\"\"\n",
    "Abstract: {abstract}\n",
    "Tags: {tags}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\", \"tags\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples = list(example_selector.select_examples(3)),\n",
    "    example_prompt=prompt,\n",
    "    prefix = \"Your task is to extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\",\n",
    "    suffix = \"Abstract: {input}\\nTags:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\" \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot prompt\n",
    "zero_shot_template = \"\"\"\n",
    "Your task is extract model names from machine learning paper abstracts. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "\n",
    "Abstract: {abstract}\n",
    "Tags:\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\"],\n",
    "    template=zero_shot_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency with Zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call as a batch to get separate responses from the model\n",
    "\n",
    "# zero-shot prompt\n",
    "self_consistency_template = \"\"\"\n",
    "Your task is extract model names from machine learning paper abstracts delimited by ```. Your response is an an array of the model names in the format [\\\"model_name\\\"]. If you don't find model names in the abstract or you are not sure, return [\\\"NA\\\"]\n",
    "\n",
    "Let's think step by step: <steps>\n",
    "Abstract: ```{abstract}```\n",
    "Tags (should just output the model names in an array):\n",
    "\"\"\"\n",
    "\n",
    "self_consistency_prompt = PromptTemplate(\n",
    "    input_variables=[\"abstract\"],\n",
    "    template=self_consistency_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chains\n",
    "\n",
    "zero_chain = LLMChain(llm=llm, prompt=zero_shot_prompt)\n",
    "few_shot_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "self_consistency_chain = LLMChain(llm=llm, prompt=self_consistency_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the abstracts from val_data\n",
    "abstracts = [{\"input\": val_data[i][\"abstract\"]} for i in range(len(val_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot predictions\n",
    "few_shot_predictions = few_shot_chain.apply(abstracts)\n",
    "\n",
    "# zero-shot predictions\n",
    "zero_shot_predictions = zero_chain.apply(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \" ['Evol-Instruct', 'WizardLM', 'ChatGPT']\"},\n",
       " {'text': \" ['FLAN-T5', 'LoRA']\"},\n",
       " {'text': \" ['NA']\"},\n",
       " {'text': \" ['PAXQA', 'QG', 'lexically-constrained machine translation']\"},\n",
       " {'text': \" ['ChatGPT']\"},\n",
       " {'text': \" ['ViT', 'OpenCLIP']\"},\n",
       " {'text': \" ['Inpaint Anything (IA)', 'Segment-Anything Model (SAM)', 'Stable Diffusion', 'AIGC']\"},\n",
       " {'text': \" ['BLIP', 'Segment-Anything', 'text-to-image diffusion']\"},\n",
       " {'text': \" ['Chameleon', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\"},\n",
       " {'text': \" ['NA']\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"text\": '[\"ChatGPT\"]'}, {\"text\": '[\"FLAN-T5\", \"AMR2.0\", \"AMR3.0\", \"BioAMR\", \"LoRA\"]'}, {\"text\": '[\"NA\"]'}, {\"text\": '[\"Question Generation (QG) Model\", \"Annotation Projection\", \"Lexically-Constrained Machine Translation\", \"Extractive QA Models\"]'}, {\"text\": '[\"ChatGPT\"]'}, {\"text\": '[\"ViT model\", \"OpenCLIP\"]'}, {\"text\": '[\"Segment-Anything Model (SAM)\", \"Stable Diffusion\", \"AIGC models\"]'}, {\"text\": '[\"BLIP Model\", \"Segment-Anything Model\", \"Text-to-Image Diffusion Model\"]'}, {\"text\": '[\"Chameleon\", \"GPT-4\", \"ChatGPT\"]'}, {\"text\": '[\"NA\"]'}]\n"
     ]
    }
   ],
   "source": [
    "# self-consistency predictions\n",
    "num_of_samples = 5\n",
    "len_of_abstracts = len(abstracts)\n",
    "output_format = \"\"\"[{\"text\": '[<model_names>]'}, {\"text\": '[<model_names>]'}, ...]\"\"\"\n",
    "\n",
    "# number of time to produce tags for each abstract\n",
    "self_consistency_samples = [self_consistency_chain.apply(val_data) for i in range(num_of_samples)]\n",
    "\n",
    "def get_model_names(samples):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are given an array of outputs (delimited by ####) generated by a model. \\n\\nThe outputs contain a list of model names the model generated for {len_of_abstracts} different abstracts. There are {num_of_samples} different samples in the data.\\n\\nYour task is to pick the most common and consistent output from the {num_of_samples} different samples, corresponding to each of the 10 different abstracts. \\n\\nOutput format should be {output_format}\"\"\".format(len_of_abstracts=len_of_abstracts, num_of_samples=num_of_samples, output_format=output_format)\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"####\\n{samples}\\n####\"\"\".format(samples=self_consistency_samples)\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# get the model names\n",
    "self_consistency_predictions = get_model_names(self_consistency_samples)\n",
    "\n",
    "print(self_consistency_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert self_consistency_predictions to list of objects\n",
    "\n",
    "final_self_consistency_predictions = eval(self_consistency_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation chain from LangChain (using an LLM to evaluate)\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "zero_shot_graded_outputs = eval_chain.evaluate(val_data, zero_shot_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")\n",
    "\n",
    "fw_graded_ouputs = eval_chain.evaluate(val_data, few_shot_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")\n",
    "\n",
    "self_consistency_graded_outputs = eval_chain.evaluate(val_data, final_self_consistency_predictions, question_key=\"abstract\", prediction_key=\"text\", answer_key=\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': ' INCORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' CORRECT'},\n",
       " {'results': ' INCORRECT'},\n",
       " {'results': ' CORRECT'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Real Answer: ['LLaMA', 'ChatGPT', 'WizardLM']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"WizardLM\", \"LLaMA\", \"ChatGPT\", \"Evol-Instruct\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Evol-Instruct', 'WizardLM', 'ChatGPT']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"ChatGPT\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Real Answer: ['FLAN-T5', 'FLAN']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"FLAN-T5\", \"AMR\", \"UD\", \"SRL\", \"LoRA\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['FLAN-T5', 'LoRA']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"FLAN-T5\", \"AMR2.0\", \"AMR3.0\", \"BioAMR\", \"LoRA\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 2:\n",
      "Real Answer: ['NA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"NA\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['NA']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"NA\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 3:\n",
      "Real Answer: ['PAXQA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: [\"PAXQA\", \"QG model\", \"annotation projection\", \"lexically-constrained machine translation\", \"extractive QA models\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['PAXQA', 'QG', 'lexically-constrained machine translation']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"Question Generation (QG) Model\", \"Annotation Projection\", \"Lexically-Constrained Machine Translation\", \"Extractive QA Models\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 4:\n",
      "Real Answer: ['ChatGPT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: [\"ChatGPT\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['ChatGPT']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"ChatGPT\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 5:\n",
      "Real Answer: ['OpenCLIP', 'ViT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: [\"ViT model\", \"OpenCLIP\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['ViT', 'OpenCLIP']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"ViT model\", \"OpenCLIP\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 6:\n",
      "Real Answer: ['SAM', 'IA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"Segment-Anything Model (SAM)\", \"Stable Diffusion\", \"AIGC models\", \"Inpaint Anything (IA)\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Inpaint Anything (IA)', 'Segment-Anything Model (SAM)', 'Stable Diffusion', 'AIGC']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"Segment-Anything Model (SAM)\", \"Stable Diffusion\", \"AIGC models\"]\n",
      "Self-consistency Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 7:\n",
      "Real Answer: ['Anything-3D', 'BLIP', 'Segment-Anything']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"BLIP model\", \"Segment-Anything model\", \"text-to-image diffusion model\", \"Anything-3D\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['BLIP', 'Segment-Anything', 'text-to-image diffusion']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"BLIP Model\", \"Segment-Anything Model\", \"Text-to-Image Diffusion Model\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 8:\n",
      "Real Answer: ['Chameleon', 'GPT-4', 'ChatGPT']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"GPT-4\", \"ChatGPT\", \"ScienceQA\", \"TabMWP\"]\n",
      "Zero-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['Chameleon', 'GPT-4', 'ScienceQA', 'TabMWP', 'ChatGPT']\n",
      "Few-shot Predicted Grade:  INCORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"Chameleon\", \"GPT-4\", \"ChatGPT\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Example 9:\n",
      "Real Answer: ['NA']\n",
      "----------------------\n",
      "Zero-shot Predicted Answer: \n",
      "[\"NA\"]\n",
      "Zero-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Few-shot Predicted Answer:  ['NA']\n",
      "Few-shot Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "Self-consistency Predicted Answer: [\"NA\"]\n",
      "Self-consistency Predicted Grade:  CORRECT\n",
      "----------------------\n",
      "\n",
      "\n",
      "Zero-shot Accuracy: 0.4\n",
      "Few-shot Accuracy: 0.3\n",
      "Self-consistency Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "zero_shot_correct = 0\n",
    "fw_correct = 0\n",
    "self_consistency_correct = 0\n",
    "\n",
    "for i, eg in enumerate(val_data):\n",
    "    print(f\"Example {i}:\")\n",
    "    #print(\"Question: \" + eg['abstract'])\n",
    "    print(\"Real Answer: \" + str(eg['tags']))\n",
    "    print(\"----------------------\")\n",
    "    print(\"Zero-shot Predicted Answer: \" + zero_shot_predictions[i]['text'])\n",
    "    print(\"Zero-shot Predicted Grade: \" + zero_shot_graded_outputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"Few-shot Predicted Answer: \" + few_shot_predictions[i]['text'])\n",
    "    print(\"Few-shot Predicted Grade: \" + fw_graded_ouputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"Self-consistency Predicted Answer: \" + final_self_consistency_predictions[i]['text'])\n",
    "    print(\"Self-consistency Predicted Grade: \" + self_consistency_graded_outputs[i]['results'])\n",
    "    print(\"----------------------\")\n",
    "    print(\"\\n\")\n",
    "    # track the number of \"Correct\" grades for each model\n",
    "    if zero_shot_graded_outputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        zero_shot_correct += 1\n",
    "    if fw_graded_ouputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        fw_correct += 1\n",
    "    if self_consistency_graded_outputs[i]['results'].strip(\" \") == \"CORRECT\":\n",
    "        self_consistency_correct += 1\n",
    "\n",
    "print(\"Zero-shot Accuracy: \" + str(zero_shot_correct/len(val_data)))\n",
    "print(\"Few-shot Accuracy: \" + str(fw_correct/len(val_data)))\n",
    "print(\"Self-consistency Accuracy: \" + str(self_consistency_correct/len(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: LangChain's built-in grading prompt template is not optimal and we have developed a more optimal one for this use case in the course. As an exercise, try to integrate that prompt to the grader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
